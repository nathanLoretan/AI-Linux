- Default of CFS and how to improve it?
	- consider priority and previous running time
	- scheduled more often regarding the priority and/or small delta exec time
	- delta_exec_weighted = delta_exec * (NICE_0_LOAD / curr->load.weight)
	  curr->vruntime += delta_exec_weighted;
	- when a process wakes up:
	= (process virtual runtime) â€“ (run tree minimum virtual runtime)

	BUT

	- don't consider that may be let running a process before another could
	  increase its running time because it will not be blocked

	- don't consider the task not running blocked because of a semaphore

	- don't consider the process that was blocked

- Default of load balance and how to improve it?
- Default of LRU and how to improve it?

--------------------------------------------------------------------------------

Learning Technics:
	C4.5				-> decision tree, supervised
	K-NN				-> classification per regression
	Trees, Lazy, Rules
	Decision Trees
	PAC learning
	SVM
	random forests
	CN2

Error:
	CC
	RAE

Instruction per cycle measurement
Cache miss measurement
get number of switching context
radix tree
blackboard communication strategy

--------------------------------------------------------------------------------

/!\ value function approximation with multi-agent system

- could be still useful to have a dirty weight bit in some case

TOLEARN:
--------
intel -> rdpmu
chain monte carlo and MDP
advantage function: A(s, a) = Q(s, a) - V(s) -> certainly need it
learning using batch
experience replay
function approximation
If a page in disk is always the same page load in RAM ??

TODO:
-----
- [1] try to use policy gradient
	-> nope, policy gradient use probability to perform action not heuristic
	   selection
	-> most of the policy gradient update methods need more complex calculation

- [1] rethink the utilization target approximation

	-> priority of process i to run
		task with higher priority should run more often because they ask more
		resource

	-> the task has a semaphore granted and block the other one
		If the task has lock, it should be run quickly to remove other tasks from
		the wait queue

	XX time left to process i to finish his tick
		NOT REALLY USEFULL, already considered with the time distributed by each
		process when calculating U

	-> nbr time blocked, exclusive
		if a process is not often scheduled because it has a low priority and
		each time it runs it is quickly blocked to access a resources,
		the process will never move ahead its task

		may be better let it run often for shorter time, or let it run at first
		could let it get access to the resource without being blocked

	-> nbr time sleep, non exclusive
		if the process always go sleep during his time. Better to let him go
		first to not have to get worried about it anymore

- [1] rethink the rewards
	-> Rprev - Rnew, rewards better when the agent took a good action
	                 the execution time between the process is well distributed

    -> if the process goes sleep -> remove the state of the process
	-> if the process is blocked -> keep the state of the process

- [2] rethink the rewards
	->

- [4] rethink the rewards
	->

PROBLEM:
--------
- learning with process scheduler => no need of communication among the queue
	-> each run-queue learn the weight and after a tick, the best run-queue
	   give the weight to the other
	-> average with ranking on how good the run-queue is performing, use dw

- learning with process migration => done during load_balance_tick()
	-> load balance done at load balance tick for each Core
	-> use avg. dw over the different Core
	-> If moving current process, need to reschedule

- learning with page reclaiming
	-> use dirty bit, when weights updated
	-> can only re-update with a utility than don't have the dirty bit
