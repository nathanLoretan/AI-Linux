NOTE: several cores = several agents

Multi-State Q-learning Approach for the Dynamic Load Balancing
--------------------------------------------------------------
Use a single agent approach
Q-Learning

LP = Logical process

load-balancing algorithm:
    - computation (NOTE: process-bound)
    - communication (NOTE: I/O bound)

Use for Q-Learning:
    - choice of dynamic load-balancing algorithm
    - percentage of nodes which participate in the load balancing
    - number of LP which are transferred from one node to another

Machine Learning Based Online Performance Prediction for Runtime Parallelization
---------------------------------------------------------------------------------
task parallelism and data parallelism.
performs models derived from past performance data collected
Use ANN
Master and Worker

Analyzer -> dependence analysis
performance modeler -> predicts task computation and data communication costs
scheduler -> static algorithm

ANN input = data size of function input parameters
ANN output = predicted function execution time and the predicted output data size


Reinforcement Learning for Operating Systems Scheduling NOTE: Migration thread
--------------------------------------------------------
If high priority job is scheduled with ‘bad’ co-runners ?
Unbalanced core assignment ?
Traditional schedulers employ a fixed policy
MDP

maintain states per core
State variables:
    - average normalized instruction per cycle
    - average cache affinity
    - average cache miss rate

Actions:
    - migrates kth thread from core i to core j

Rewards, defined per state, per core:
    - Instruction rate in the ith core, in state st during the interval (t, t+1)

Core assignment balance (CAB)
Response time fairness (RTF)

Operating System Scheduling On Heterogeneous Core Systems
---------------------------------------------------------
Use reinforcement learning for optimized assignment policy. Learning is required
for finding the optimal assignment policy because the system’s state changes
stochastically over time.

each system core i learns the function Bi -> approximate the expected future
normalized IPC on a core

State of cores depends on:
- average normalized IPC -> IPC = instruction per cycle
- average cache affinity -> if a thread has executed on the core i within the
  last interval, cache affinity is 1. otherwise 0
- average cache miss rate of threads

Action: moving thread k from core i to j
