TIMESLICE
=========
sched_slice()                                           smart.c
        __sched_period()                                smart.c

Modified, return a constant timeslice CONFIG_SCHED_SFS_TIMESLICE no more
          __sched_period()

RQ INIT
=======

sched_init()                                            core.c
        for_each_possible_cpu() ->
                init_cfs_rq()                           smart.c
        init_synch_point()

Modified, Initialize the different factors and weights used by the AI and
          initialize the list that store the entity

CREATE NEW PROCESS
==================

kernel_thread()                                         fork.c
  do_fork()

do_fork()                                               fork.c
  _do_fork()
          copy_process()
                          dup_task_struct()       -> create the task
                          sched_fork()            core.c
          wake_up_new_task()                      core.c
                  activate_task()                 core.c

sched_fork()                                      core.c -> set rq of se
        __sched_fork()                            -> set sched_entity
        __set_task_cpu()
                set_task_rq()
        task_fork_smart()                         smart.c
                init_aistats()
                if curr -> update_curr
                place_entity()                    TODO
                normalize state

Modified, initialize the aistats structure of the new state, give the state
          of the parent process and normalize the state

ADD TASK IN QUEUE
=================

activate_task()                                                 core.c
        enqueue_task()
                enqueue_task_smart()                            smart.c
                        enqueue_entity()                        -> initialise vruntime of the process
                                if curr -> normalize
                                update_curr()
                                if !curr & !sleep -> normalize
                                if !state_used -> add_aistats()
                                if !curr -> __enqueue_entity()     -> entity not in rq
                                se->on_rq = 1

Modified,

REMOVE TASK QUEUE
=================

deactivate_task()                                               core.c
        dequeue_task()
                dequeue_task_smart()                            smart.c
                        dequeue_entity()
                                update_curr()
                                if !state_used -> remove_aistats()  -> entity still in rq
                                if !curr -> __dequeue_entity()
                                if !sleep -> Normalize state
                                se->on_rq = 0
                        set_next_buddy()

QUESTION: What call deactivate_task() ?

When migrating process:
detach_task()
        deactivate_task()

When scheduling, if !preempt && task not running (TASK_RUNNING = 0) and
                    !signal_pending_state(prev->state, prev)
__schedule()
        deactivate_task()

signal_pending_state(prev->state, prev)
        return 0 if state not (TASK_INTERRUPTIBLE | TASK_WAKEKILL)
        return 0 if TASK_INTERRUPTIBLE and signal not yet received

Modified,

UPDATE CURRENT ENTITY STATE AND RUNQUEUE STATE
==============================================

scheduler_tick()                                                core.c
        task_tick_smart()                                       smart.c
                entity_tick()
                        update_curr()
                                update_aistats()
                                update_state()
                                update_rewards()
                        resched_curr()

hrtick_start_fair()                                             smart.c
        sched_slice()
        hrtick_start() -> start time slice                      core.c

hrtick -> after finishing timeslice                             core.c
        task_tick_smart()                                       smart.c



Modified, update the state of the entity and the runqueue and set schedul
          flag

SELECT NEW PROCESS
==================

resched_curr()  -> set TIF_NEED_RESCHED

schedule()                                                      core.c
        __schedule()
                pick_next_task()
                        pick_next_task_smart()                  smart.c
                                if curr->on_rq -> update_curr()
                                pick_next_entity()
                                        update_weights()
                                        update_next()

                		put_prev_entity()
                                        if still runnable
                                                update_curr()
                                                _enqueue_entity()

                                set_next_entity()
                                        _dequeue_entity()

Modified, Calculate the utility of the next task and select it to run next

/!\ When selecting new process, remove_aistats and add_aistats are not called

SEMAPHORE and READ/WRITE SEM
============================

down()                                                          semaphore.c
        __down()
                __down_common()
                        __set_current_state(TASK_UNINTERRUPTIBLE)
                        list_add_tail(wait_list)
                        block_count++
                        schedule_timeout()                      timer.c
                                schedule()                      core.c
        lock_count++

NOTE: same for down_interruptible() down_killable() down_timeout()
NOTE: not for down_trylock() -> no wait

down_read()                                                     rwsem.c
        __down_read()
                __down_read_common()                            rwsem-spinlock.c
                        block_count++
                        list_add_tail(wait_list)
                        schedule()                              core.c
        lock_count++

NOTE: same for down_read_killable() down_write() down_write_killable()
NOTE: not for down_read_trylock() and down_write_trylock() -> no wait

up()                                                            semaphore.c
        __up()
                list_first_entry(wait_list)
                wake_up_process()                               core.c

up_read()                                                       rwsem.c
        __up_read()                                             rwsem-spinlock.c
                __rwsem_wake_one_writer()
                        list_entry()
                wake_up_process()

up_write()                                                      rwsem.c
        __up_write()                                            rwsem-spinlock.c
                __rwsem_do_wake()
                         list_entry()
                wkae_up_process()

Modified, add lock_count++ and block_count++

MUTEX
=====

NOTE: slowpath == sleep if fast path doesn't work

mutex_lock()                                                    mutex.c
        __mutex_lock_slowpath()
                __mutex_lock()
                        __mutex_lock_common()
                                list_add_tail(waiter.list)
                                block_count++
        lock_count++

NOTE: same for mutex_lock_interruptible() mutex_killable()
NOTE: not for mutex_trylock()

mutex_unlock()                                                    mutex.c
        __mutex_unlock_slowpath()
                list_first_entry()
                wake_up_add()
                wake_up_q()

Modified, add lock_count++ and block_count++

SPIN-LOCK
=========

spin_lock()                                                     spinlock.h
        raw_spin_lock()
                _raw_spin_lock()                                spinlock.c
                        __raw_spin_lock()                       spinlock_api_smp.h
                                do_raw_spin_lock()              spinlock_debug.c

NOTE: same for spin_lock_irqsave() spin_lock_bh() write_lock() read_lock()

spin_unlock()                                                   spinlock.h
        raw_spin_unlock()
                __raw_spin_unlock()                             spinlock_api_smp.h
                        do_raw_spin_unlock()                    spinlock_debug.c

NOTE: same for spin_unlock_irqrestore() spin_unlock_bh() write_unlock() read_unlock()

Modified, None

SLEEP
=====

udelay()        -> busy_wait_loop

usleep_range()                                          timer.c
        sleep_count++
        __set_current_state(TASK_UNINTERRUPTIBLE)
        schedule_hrtimeout_range()                      hrtimer.c
                ...
                        enqueue_hrtimer()
                                timerqueue_add()        timerqueue.c
                schedule()


        remove_hrtimer                                  hrtimer.c
                timerqueue_del                          timerqueue.c

msleep()                                                timer.c
        sleep_count++
        schedule_timeout_uninterruptible()
                __set_current_state(TASK_UNINTERRUPTIBLE)
        	schedule_timeout(timeout)

msleep_interruptible()                                  timer.c
        sleep_count++
        schedule_timeout_interruptible()
                __set_current_state(TASK_INTERRUPTIBLE)
                schedule_timeout(timeout)

schedule_timeout() -> callback function process_timeout
         process_timeout()                              timer.c
                wake_up_process()                       core.c

Modified, add sleep_count++

WAKE UP
=======

wake_up_process()                                               core.c
        try_to_wake_up()
                ttwu_remote() if p->on_rq
                        ttwu_do_wakeup()
                                p->state = TASK_RUNNING

                ttwu_queue()
                        ttwu_do_activate()
                                ttwu_activate()
                                        activate_task()
                                ttwu_do_wakeup()
                                        p->state = TASK_RUNNING

Modified, None

KILL
====

do_exit()                                               exit.c
        set_current_state(TASK_UNINTERRUPTIBLE)
        schedule()
                __schedule()
                        deactivate_task()

do_group_exit()
        do_exit()

Modified, None

YIELD
=====

yield()                                                 core.c
        do_sched_yield()
                yield_task_fair()                       smart.c
                        update_curr()
                        set_skip_buddy()
                schedule()                              core.c

yield_to()                                              core.c
        yield_to_task_fair()                            smart.c
                yield_task_fair()

Modified, None

-------

put_prev_task_fair()                                    smart.c
        put_prev_entity()

set_curr_task_fair()                                    smart.c
        set_next_entity()

Modified, None

-------

static void remove_aistats(struct cfs_rq *cfs_rq, struct sched_entity *old)
static void add_aistats(struct cfs_rq *cfs_rq, struct sched_entity *new)

TODO:   . Initialize AI in runqueue                    -> init_cfs_rq()                OK
        . Initialize sched_entity in new process       -> task_fork_smart()            OK
        . Update environement when deleting a task     -> dequeue_entity()             OK
        . Update environement when adding a task       -> enqueue_entity()             OK
        . Change group                                                                 OK
                -> switched_from() & task_change_group()
                => detach_task_cfs_rq & attach_task_cfs_rq
        . Calculate the utility of the next task       -> pick_next_task_smart()       OK
        . Update the environment                       -> update_curr()                OK
                - creating      OK
                - deleting      OK
                - sleeping      OK
                - blocking      OK
                - scheduling    OK
                - wake-up       OK
        . Set attributes with macro:
                scheduled_count                         -> smart.c      OK
                        pick_next_task_smart()

                runtime_sum                             -> smart.c      OK
                        update_aistats()

                state                                   -> smart.c      OK
                        update_state()
                        enqueue_entity()
                        dequeue_entity()
                        task_fork_smart

                sleep_count                             -> timer.c      OK
                        usleep_range()
                        msleep()
                        msleep_interruptible()
                        yield_task_smart()

                lock_count
                        down()                          -> semaphore.c  OK
                        down_interruptible()
                        down_killable()
                        down_trylock()
                        down_timeout()

                        down_read()                     -> rwsem.c      OK
                        down_read_killable()
                        down_read_trylock()

                        down_write()                    -> rwsem.c      OK
                        down_write_killable()
                        down_write_trylock()

                        mutex_lock()                    -> mutex.c      OK
                        mutex_lock_interruptible()
                        mutex_lock_killable()
                        mutex_trylock()

                block_count, only the added to the wait list            OK
                        __down_common()                 -> semaphore.c
                        __down_read_common()            -> rwsem-spinlock.c
                        __down_write_common()
                        __mutex_lock_common()           -> mutex.c

        . Average, Calcul, Fixed Point, Overflow                        OK

                update_rewards()        OK
                update_aistats()        OK
                update_weights()        OK
                update_next()           OK

                add(a, b, ta, tb) 	 a + b
                sub(a, b, t1, t2)  	 a - b
                mul(a, b, t1, t2)	(a * b) >> Q
                div(a, b, t1, t2)	(a << Q) / b

                Type:
                u32     ->      0 to 4,294,967,295
                s32     ->      -2,147,483,648 to 2,147,483,647
                u64     ->      0 to 18,446,744e12
                s64     ->      -9,223,372e12 to 9,223,372e12

                What to scale:
                x avg_runtime   x alpha         x w_load        x rewards
                x avg_block     x gamma         x w_avg_block   x ut
                x avg_sleep     x q_value       x w_avg_sleep   x ut_sum
                x avg_lock      x old_q_value   x w_avg_lock

                Type definition: all s64
                - alpha                         - scheduled_count
                - gamma                         - state
                - w_load                        - old_state
                - w_avg_block
                - w_avg_sleep                   - avg_runtime
                - w_avg_lock                    - runtime_sum
                - state                         - avg_block
                - q_value                       - block_count
                - old_q_value                   - avg_sleep
                - ut_sum                        - sleep_count
                - update                        - avg_lock
                - rewards                       - lock_count
                - min_start_state               - di
                - min_state                     - ut

                overflow:
                        scheduled_count
                        runtime_sum
                        block_count
                        sleep_count
                        lock_count

        . place_entity()                        OK
        . Remove everythings with vruntime
                check_spread()                  just for debug
                        enqueue_entity()
                        put_prev_entity()
                check_preempt_tick()
                        entity_tick()
                migrate_task_rq_smart()         OK
                wakeup_preempt_entity()         OK
                        pick_next_entity()      TODO next, skip, last buddies
                        check_preempt_wakeup()
                vruntime_normalized()           OK
                        detach_task_cfs_rq()
                        attach_task_cfs_rq()
                max_vruntime()                  OK
                        update_min_vruntime()
                        place_entity()
                min_vruntime()                  OK
                        update_min_vruntime()
                        place_entity()
                update_min_vruntime()           OK
                        update_curr()
                        dequeue_entity()

        . calc_delta_fair()                     OK useless
                calculate the virtual delta time regarding the weight of the
                entity
        . check_preempt_tick()                  OK
                Check if the process should be preempted if it has finished its
                timeslice

                TODO: need if scheduler_tick != timeslice

        . Timeslice                             OK
                no fixed timeslice-> calculated at runtime
                between sysctl_sched_min_granularity and sysctl_sched_latency
                                        750,000ns             6,000,000ns

                __sched_period()
                        determine a period in which each task runs once.
                                if nr > 8
                                        nr * 750,000ns
                                else
                                        6,000,000ns

                . sched_slice()
                        based on the period to run each process return by
                        __sched_period(), split the time among the task
                        regarding their load

                        sched_vslice()          OK
                        check_preempt_tick()    OK
                        hrtick_start_fair()
                        get_rr_interval_fair()

                . sched_vslice()                        OK
                        return the virtual timeslice of the entity
                        place_entity()

        . tick == CONFIG_HZ
                - the tick to update the process is CONFIG_HZ (250 or 1000) or
                is determine with high resolution timer with CONFIG_SCHED_HRTICK

                In scheduler_tick, rescheduling is activated with check_preempt_tick()
                which control is the entity has finished is timeslice.

        . debug
                https://lwn.net/Articles/365835/
                https://elinux.org/Kernel_Debugging_Tips
                https://www.slideshare.net/vh21/linux-kernel-tracing
                https://opensourceforu.com/2011/04/kernel-debugging-using-kprobe-and-jprobe/
                http://devarea.com/linux-kernel-development-creating-a-proc-file-and-interfacing-with-user-space/

                dmesg -n 5       -> set log level loglevel < 5
                dmesg -wH        -> follow file evolution
                tail -f filename -> follow file evolution
                watch cat ...

                printk()       -> dmesg -wH
                trace_printk() -> trace in a tracer file

                kprob:
                        kp.pre_handler = pre handler function
                        kp.post_handler = post handler function
                        kp.addr = function to debug
                        register_kprobe();
                        unregister_kprobe(&kp);

                        NOTE: Still use printk

                jprob:
                        like kprob but the handler get the same argument than
                        the function to debug to get the entry arguments

                        jp.kp.addr = function to debug
                        jp.entry = handler
                        jprobe_return(); when returning from handler
                        register_jprobe(&my_probe);
                        unregister_jprobe(&my_probe);

                Create Proc file:

                        struct file_operations
                                .read
                                .write
                                .open
                        proc_create()
                        proc_remove()

                        rewrite sched/debug.c with a new proc for the AI

                create new debug file ???
                create a new trace    ???

        . synchronization_point and batch                       OK
                weight are global


                QUESTION: is timer_list per-CPU? Yes
                        https://lwn.net/Articles/22911/
                        dynamic timer is bound to the CPU that activated it
                QUESTION: is timer_list in interrupt context? Yes
                QUESTION: is it possible to lock with timer_list? Yes
                        read_lock_irq write_lock_irq
                        DEFINE_RWLOCK(x)

                QUESTION: batch methodes?
                        average dw over each update fpor each CPU the batch

        . Set kconfig:                                  OK
                -> /init/Kconfig
                -> kernel/sched/Kconfig
                https://www.kernel.org/doc/Documentation/kbuild/kconfig-language.txt
                CONFIG_SCHED_SFS                n
                CONFIG_SCHED_SFS_TIMESLICE      1000000ns
                CONFIG_SCHED_SFS_SYNCH_TIMER    1000ms
                CONFIG_SCHED_SFS_FIXP_SHIFT     10
                CONFIG_SCHED_SFS_ALPHA          0
                CONFIG_SCHED_SFS_GAMMA          0
                CONFIG_SCHED_SFS_W_LOAD         0
                CONFIG_SCHED_SFS_W_AGV_BLOCK    0
                CONFIG_SCHED_SFS_W_AGV_SLEEP    0
                CONFIG_SCHED_SFS_W_AGV_LOCK     0

                NOTE: the constant should already be scaled to the fixedpoint

        . entity_before() important to avoid any overflow       OK

                - caller:
                        __enqueue_entity
                        pick_next_entity
                        task_fork_fair

                - update min_state in update_aistats()

                        QUESTION: always minimum state in rq or current? no
                                /!\ if state overflow

                        remove_aistats  -> process in rq
                        add_aistate     -> process not in rq
                        update_ai_stats -> curr normaly not in rq

                - when calculating, sub by min_state for all entity state:
                        remove_aistats
                                r -> problem because diff
                                     but no need to subtract min_state
                        add_aistats
                                r -> problem because diff
                                     but no need to subtract min_state
                        update_rewards
                                r_old, r -> problem because diff
                                            but no need to subtract min_state
                        update_next
                                s, s2, si, si2 -> problem is state overflow
                                                  and entity state not

                To reduce the overhead of the calculation, reduce the overflow
                when detected during the update of the runqueue state.

                This required to have min always the real-min state

        . Convergence                           OK
        . Determine default weight              OK

                -> always load weight at the begining, only use it to keep
                -> fair regarding the different process
                -> other == 0

                CONFIG_SCHED_SFS_SYNCH_TIMER    1000
                CONFIG_SCHED_SFS_FIXP_SHIFT     10
                CONFIG_SCHED_SFS_ALPHA          -> 0.5     512
                CONFIG_SCHED_SFS_GAMMA          -> 0.8     820
                CONFIG_SCHED_SFS_W_LOAD         1
                CONFIG_SCHED_SFS_W_AGV_BLOCK    0
                CONFIG_SCHED_SFS_W_AGV_SLEEP    0
                CONFIG_SCHED_SFS_W_AGV_LOCK     0

        . migration task
        https://www.systutorials.com/239971/migration-thread-works-inside-linux-kernel/

                Task is called when stopping and migrating a task to another CPU.
                Thereis one process per cpu.

                The task is initialized during the early_initcall by calling the function
                sched_set_stop_task().

        . RSDTtXZPI                                     fs/proc/array.c
                "R (running)",		/* 0x00 */
        	"S (sleeping)",		/* 0x01 */    TASK_INTERRUPTIBLE
        	"D (disk sleep)",	/* 0x02 */    TASK_UNINTERRUPTIBLE
        	"T (stopped)",		/* 0x04 */
        	"t (tracing stop)",	/* 0x08 */
        	"X (dead)",		/* 0x10 */
        	"Z (zombie)",		/* 0x20 */
        	"P (parked)",		/* 0x40 */
                "I (idle)",		/* 0x80 */

        . set_last_buddy        -> when preempted
        . set_next_buddy        -> when preempted
        . set_skip_buddy        -> when yield

        . QUESTION: When is a task in idle state ???
                 TASK_IDLE -> I
                        set_current_state(TASK_IDLE)
                        __set_current_state(TASK_IDLE)
                        prepare_to_wait(... TASK_IDLE)
                        ___swait_event(... TASK_ILDE ...)
                        ___wait_event(... TASK_ILDE ...)

        . QUESTION: When is a task in sleep state other than blocked ???

                TASK_UNINTERRUPTIBLE -> D

                        prepare_to_wait_exclusive()                     wait.c
                                __add_wait_queue_entry_tail()
                                set_current_state()

                        prepare_to_wait()                               wait.c
                                __add_wait_queue_entry_tail()
                                set_current_state()

                        prepare_to_wait_event()                         wait.c
                                __add_wait_queue_entry_tail()
                                set_current_state()

                        wait_on_bit
                        wait_on_bit_timeout
                        wait_on_bit_lock_io
                                ...
                                        prepare_to_wait_exclusive()
                                        __add_wait_queue_entry_tail()

                        ___swait_event
                                prepare_to_swait_event
                                        prepare_to_swait
                                                __prepare_to_swait
                                                        list_add

                        ___wait_var_event
                        ___wait_event
                                prepare_to_wait_event()
                                        __add_wait_queue_entry_tail()
                                        __add_wait_queue()
                                set_current_state()


                        wait_for_common()                               completion.c
                        wait_for_common_io()
                        wait_for_completion
                        wait_for_completion_timeout
                                __wait_for_common()
                                        do_wait_for_common()
                                                __add_wait_queue_entry_tail_exclusive()
                                        __set_current_state(state);

                        wait_woken                                      wait.c
                                set_current_state()
                                schedule_timeout()

                TASK_INTERRUPTIBLE -> S
                        prepare_to_swait
                        prepare_to_wait_exclusive
                        prepare_to_wait
                        set_current_state
                        __set_current_state
                        wait_woken
                        wait_on_bit

                schedule_timeout()
                __add_wait_queue                                wait.h
                __add_wait_queue_exclusive                      wait.h
                __add_wait_queue_entry_tail                     wait.h
                __add_wait_queue_entry_tail_exclusive           wait.h

                If task always waiting, the execution avg. time will be reduce and
                will influence the decision, no necessarily needed to add sleep and wait.

                =========================================================================
                TODO:
                . Save a specific load with the right shift in the aistats
                . rename dq_value and update
                . use the prev load, sleep avg, block avg, lock avg


https://idea.popcount.org/2012-12-11-linux-process-states/
https://elixir.bootlin.com/linux/v4.17.2/source/include/linux/sched.h#L92
Debugging:

        - rescale, Q, dQ, State, Rewards

  	printk("Debug=========================================================\n");
  	printk("Process: %s\n", p->comm);
  	printk("Debug=========================================================\n");

        #include <linux/sched/ai.h>

        #ifdef CONFIG_SCHED_SFS
        	ai_inc(current->se.aistats.sleep_count);
        #endif

Scaled or not scaled, That is the question:

        aistats:
        --------
        u64 scheduled_count     -> not

        u64 state               -> reduced
        u64 old_state;          -> reduced

        u64 avg_runtime;        -> reduced
        u64 runtime_sum;        -> reduced

        u64 avg_block;          -> scaled
        u64 block_count;        -> not

        u64 avg_sleep;          -> scaled
        u64 sleep_count;        -> not

        u64 avg_lock;           -> scaled
        u64 lock_count;         -> not

        u64 di;                 -> NOT USED !!!
        s64 ut;                 -> scaled
        s64 targ;               -> scaled
        s64 q_value;            -> scaled

        ai:
        ---
        s64 dw_load;            -> scaled
        s64 dw_avg_block;       -> scaled
        s64 dw_avg_sleep;       -> scaled
        s64 dw_avg_lock;        -> scaled
        u64 batch_cnt;

        u64 state;              -> reduced
        u64 min_state;          -> reduced

        s64 q_value;            -> scaled
        s64 dq_value;           -> scaled
        s64 old_q_value;        -> scaled

        s64 ut_sum;             -> scaled

        s64 rewards;            -> scaled and reduced

        update_rewards():
        -----------------
        s64 dij                 -> not
        s64 dij2                -> not
        s64 rewards;            -> not
        u64 min_state;          -> not

        update_next():
        --------------
        s64 s                   -> scaled       based on ai->state
        s64 s2                  -> scaled       based on ai->state + avg_runtime
        s64 si                  -> scaled       based on aistats->state
        s64 si2                 -> scaled       based on aistats->state + avg_runtime
        s64 fi_s                -> scaled       based on targ, s and si
        s64 fi_s2               -> scaled       based on targ, s2 and si2

        update_dq(): -> same than for update_next()
        ------------
        s64 s                   -> scaled
        s64 s2                  -> scaled
        s64 si                  -> scaled
        s64 si2;                -> scaled
        s64 fi_s                -> scaled
        s64 fi_s2;              -> scaled

        update_weights():
        -----------------
        s64 update              -> scaled       based on alpha dq_value rewards
                                                         old_q_value gamma
                                                         q_value

        synch_point():
        --------------
        dw_load                 -> scaled
        dw_avg_block            -> scaled
        dw_avg_sleep            -> scaled
        dw_avg_lock             -> scaled
        cpu_cnt                 -> not


================================================================================
================================================================================
================================================================================
================================================================================
https://www.systutorials.com/240717/load-balancing-work-internal-operating-systems/

TRIGGER BALANCE:
================
scheduler_tick()                                                core.c
        trigger_load_balance()                                  fair.c
                if (time_after_eq(jiffies, rq->next_balance))
                        raise_softirq(SCHED_SOFTIRQ)            softirq.c

SCHEDULE NEXT BALANCE: TODO
======================
update_next_balance()
        get_sd_balance_interval(sd, 0)

CALLBACK:
=========
init_sched_fair_class()                                         fair.c
        open_softirq(SCHED_SOFTIRQ, run_rebalance_domains)      softirq.c

NOTE: The callback function of SCHED_SOFTIRQ is run_rebalance_domains()

REBALANCE:
==========
run_rebalance_domains()                                         fair.c
        update_blocked_averages()
                update_cfs_rq_load_avg()        IF CONFIG_FAIR_GROUP_SCHED
                for_each_leaf_cfs_rq_safe()     ELSE
                        update_load_avg()
        rebalance_domains()
                for_each_domain(cpu, sd)
                        load_balance()
                        sd->last_balance = jiffies
                        interval = sd->balance_interval
                        next_balance = sd->last_balance + interval

LOAD_BALANCE:
=============
load_balance()                                                  fair.c
        should_we_balance()
        find_the_busiest_group()
        find_the_busiest_queue()
        number to try to move: min(sysctl_sched_nr_migrate, nr_running)
        detach_tasks()
                while cfs_tasks not empty
                        list_last_entry(cfs_tasks)       QUESTION: Where deleted from cfs_task
                        can_migrate_task()
                        detach_task()
                                deactivate_task()
                                        ...
                                        dequeue_entity()
                                                account_entity_dequeue()
                                                        list_del_init(&se->group_node)
                        list_last_entry(env->tasks)
        attach_tasks()
                while env->tasks not empty
                        list_last_entry(env->tasks)
                        list_del_init()
                        attach_task()
                                activate_task()
                                        ...
                                        enqueue_entity()
                                                account_entity_enqueue()
                                                        list_add(&se->group_node, &rq->cfs_tasks)

NOTE: sysctl_sched_nr_migrate = 32

UPDATE LOAD AVG SE:
===================
 update_load_avg()                                                     faire.c
        __update_load_avg_se()
                ___update_load_sum()
                ___update_load_avg()
        update_cfs_rq_load_avg()
                __update_load_avg_cfs_rq()
                        ___update_load_sum()
                        ___update_load_avg()
        propagate_entity_load_avg()
        update_tg_load_avg()

NOTE: tg -> Task group
NOTE: Called by enqueue_entity
                dequeue_entity
                dequeue_task_fair
                enqueue_task_fair
                put_prev_entity
                set_next_entity
                entity_tick
                attach_entity_cfs_rq
                detach_entity_cfs_rq
                sched_group_set_shares
                propagate_entity_cfs_rq
                update_blocked_averages

FIND_BUSIEST_GROUP: TODO: Select which of those condition should be kept
===================
find_busiest_group()                                                    fair.c
        update_sd_lb_stats()
                do {
                        sg = sg->next;
                } while (sg != env->sd->groups);

        check_asym_packing()

        no balance if no busiest or busiest nr_running == 0     -> out balance  KEEP
        calculate domain avg load scaled to capacity
        check if group considered to be imbalanced              -> force balance
        if this cpu is ilde and busiest no capacity             -> force balance
        no balance if this cpu more load that busiest found     -> out balance
        no balance if this cpu above domain average weight      -> balance
        if this cpu idle
                if no imbalance                                 -> out balanced

        force_balance
                calculate_imbalance()
                return busiest
        out_balance
                env_imbalance = 0


FIND_BUSIEST_QUEUE:
===================
find_busiest_queue()                                                    fair.c
        for_each_cpu_and()
                get cpu rq
                get classify (all, regular, remote)
                get capactiy = cpu->cpu_capacity
                weighted_cpuload()
                        cfs_rq_runnable_load_avg()
                                cfs_rq->avg.runnable_load_avg
                select it if:
                        nr_running > 1
                        wl > env->imbalance
                        check_cpu_capacity()

                compare with previous busiest found, keep the best

QUESTION: env->imbalance, calculate_imbalance, It is use somewhere else than find_busiest_queue
QUESTION: sd->imbalance_pct
QUESTION: Relation group, runqueue

NOTE: group_type = group_classify()
NOTE: sd = sched domain, tg = task group, ld = load balance
NOTE: cpu_capacity = SCHED_CAPACITY_SCALE = (1L << SCHED_CAPACITY_SHIFT)
NOTE: SCHED_CAPACITY_SHIFT = SCHED_FIXEDPOINT_SHIFT
NOTE: update_cpu_capacity() -> cpu_rq(cpu)->cpu_capacity

        . Attributes (rename weight for load balance):                           TODO
                Per Entity:
                - avg runtime                                   OK
                - nbr of time scheduled, avg                    avg_sched, interval_cnt->jiffies
                - nbr blocked                                   OK
                - avg time of completion without blocking       avg_comp, comp_count
                - IPC                                           if possible
                - Cache miss                                    if possible

                Per Runqueue:
                - avg runtime                                   sum     avg_runtime_sum
                - nbr of time scheduled, avg                    sum     avg_sched_sum
                - nbr blocked                                   sum     avg_block_sum
                - avg time of completion without blocking       sum     avg_comp_sum

                ----
                u64 prev_avg_runtime    -> per task     scaled     ok
                u64 prev_avg_sched      -> per task     scaled     ok
                u64 prev_avg_block      -> per task     scaled     ok
                u64 prev_avg_comp       -> per task     scaled     ok

                u64 avg_runtime_sb      -> per task     scaled     ok
                u64 avg_sched           -> per task     scaled     ok
                u64 avg_block           -> per task     scaled     ok
                u64 avg_comp            -> per task     scaled     ok

                u64 last_interval       -> per task     not        ok
                u64 interval_count      -> per task     not        ok
                u64 comp_count          -> per task     not        ok
                u64 prev_block_count    -> per task     not        ok

                u64 avg_runtime_sum     -> per task     scaled     ok
                u64 avg_sched_sum       -> per task     scaled     ok
                u64 avg_block_sum       -> per task     scaled     ok
                u64 avg_comp_sum        -> per task     scaled     ok

                s64 balance             -> per task     scaled     ok
                                        -> per queue    scaled     ok

                s64 sb_dw_avg_runtime   -> per queue    scaled     ok
                s64 sb_dw_avg_sched     -> per queue    scaled     ok
                s64 sb_dw_avg_block     -> per queue    scaled     ok
                s64 sb_dw_avg_comp      -> per queue    scaled     ok

                u64 sb_batch_cnt        -> per queue    not        ok

                s64 sb_q_value          -> per task     scaled     ok
                s64 sb_old_q_value      -> per task     scaled     ok

                bool sb_learning        -> per queue    not        ok
                bool sb_dirty_weight    -> per queue    not        ok
                bool moved              -> per task     not        ok

                s64 sb_rewards          -> per task     scaled     ok

                s64 sb_w_avg_sched      -> global       scaled     ok
                s64 sb_w_avg_runtime    -> global       scaled     ok
                s64 sb_w_avg_block      -> global       scaled     ok
                s64 sb_w_avg_comp       -> global       scaled     ok

                u64 sb_conv_limit       -> global       not        ok
                u64 sb_conv_runtime     -> global       not        ok
                u64 sb_conv_sched       -> global       not        ok
                u64 sb_conv_block       -> global       not        ok
                u64 sb_conv_comp        -> global       not        ok

                s64 sb_alpha            -> global       scaled     ok
                s64 sb_gamma            -> global       scaled     ok
                ----

                The attributes are updated by:
                . update_curr()                                                 ok
                . enqueue_entity()                                              ok
                . dequeue_entity()                                              ok
                . init_aistats()                                                OK
                . init_ai()                                                     OK
                . detach_task_cfs_rq                                            ok
                . attach_task_cfs_rq                                            ok


        . load_balance()                                                        ok
                - Considering state of queue and entity up to date

                1. select queue with the worst state TODO: Group, queue
                        - group -> sum state of queue
                2. Calculate Q(s,a) for each entity and decide to move or not
                3. calculate dQ
                4. Update with rewards at the next load balance
                5. rewards:
                        - save info in entity
                        - raised a flag moved in the entity
                        - at next balanced in the new cpu
                                . calculate the rewards
                                . update dw
                                . reset moved flag

                NOTE: Using batch make no different if weight updated on new
                      CPU or old one.

                      rq->cfs_task

                QUESTION: How to determine process that called load balance ??
                        -> trigger_load_balance() is run periodically on each CPU
                        through scheduler_tick()

        . update_balance_rewards                                                OK
        . update_balance_group                                                  OK
        . update_balance_weight                                                 OK
        . detach_tasks()                                                        OK
                . when balance <= 0 what to do ?
                        -> balance = q_value -> nothing to do. stop only if
                           no more task
                . consider affinity -> can't move the task                      OK
        . attach_tasks()                                                        OK
                -> no modification

        . Snychronization for balance                                           OK
                -> dirty weight
        . init task                                                             ok
        . init queue                                                            ok
        . debug                                                                 ok
        . Makefile                                                              ok
        . Signed-unsigned                                                       TODO
        . Check scale of local variable                                         TODO
        . balance interval                                                      TODO
        . Default weight: SB (Smart Balance)                                    TODO

                -> dependent of CONFIG_SCHED_SFS
                CONFIG_SCHED_SB
                CONFIG_SCHED_SB_SYNCH_TIMER
                CONFIG_SCHED_SB_FIXP_SHIFT
                CONFIG_SCHED_SB_CONV
                CONFIG_SCHED_SB_ALPHA
                CONFIG_SCHED_SB_GAMMA
                CONFIG_SCHED_SB_ALPHA
                CONFIG_SCHED_SB_GAMMA
                CONFIG_SCHED_SB_W_...
                CONFIG_SCHED_SB_INTERVAL -> ms
        . remove useless stuff

/!\: mark task TASK_ON_RQ_MIGRATING

active_balance will be set to wake up Migration Thread on the source CPU

Useless
        should_we_balance
        find_busiest_group
        find_busiest_queue

????
        fbd_classify_rq
        get_sd_load_idx
        SD_PREFER_SIBLING
        task_h_load
        sched_feat
